#!/usr/bin/env python3
"""
Test script to examine the documents generated by the scraper.
"""

import asyncio
import os
import sys
import json

# Add the current directory to the path so we can import main
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from main import StrandsDocsScraper

async def main():
    base_url = os.getenv('DOCS_BASE_URL', 'https://strandsagents.com/latest/documentation/docs/')
    elasticsearch_url = os.getenv('ELASTICSEARCH_URL', 'http://localhost:9200')
    
    print(f"Testing scraper with base_url: {base_url}")
    
    async with StrandsDocsScraper(base_url, elasticsearch_url) as scraper:
        # Scrape all documentation
        documents = await scraper.scrape_all_sections()
        
        print(f"Total documents scraped: {len(documents)}")
        
        if documents:
            print("\nFirst 3 documents:")
            for i, doc in enumerate(documents[:3]):
                print(f"\nDocument {i+1}:")
                print(f"  Title: {doc.get('title', 'N/A')}")
                print(f"  URL: {doc.get('url', 'N/A')}")
                print(f"  Content length: {len(doc.get('content', ''))}")
                print(f"  Has URL field: {'url' in doc}")
                print(f"  Has content field: {'content' in doc}")
                
                # Check for any missing required fields
                required_fields = ['url', 'title', 'content']
                missing_fields = [field for field in required_fields if field not in doc]
                if missing_fields:
                    print(f"  Missing fields: {missing_fields}")
                
                # Show a snippet of content
                content = doc.get('content', '')
                if content:
                    snippet = content[:200] + "..." if len(content) > 200 else content
                    print(f"  Content snippet: {repr(snippet)}")
        
        # Try to index a single document to test
        if documents:
            print("\nTesting indexing of first document...")
            try:
                scraper.index_documents([documents[0]])
                print("Indexing succeeded")
            except Exception as e:
                print(f"Indexing failed: {e}")
                import traceback
                traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
